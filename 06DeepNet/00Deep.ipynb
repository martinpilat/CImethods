{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hluboké učení\n",
    "\n",
    "###Proč posledních cca 5let?\n",
    "\n",
    "Zjednodušená historie NS:\n",
    "\n",
    "- 50.-60. léta - 1 vrstva dopředných sítí\n",
    "- 80.-90. léta - algoritmus back propagation, typicky 1-2 skryté vrstvy, stovky jednotek\n",
    "- současnost - desítky vrstev, specializované jednotky\n",
    "\n",
    "Boom hlubokých architektur:\n",
    "\n",
    "- rozvoh hardware, hlavně grafických karet\n",
    "- vyřešení některých praktických problémů spojených s mnoha vrstvami - vanishing gradient, dropout\n",
    "- unsupervised preprocessing - v aplikační oblasti efektivní autoencodéry, konvoluční vrstvy, ...\n",
    "- prostorová složitost sítí s více vrstvami je menší (existují příklady úloh vyžadující na $O(n)$ jednotek pro $d$ vrstev, ale $O(2^n)$ jednotek pro $d-1$ vrstev\n",
    "\n",
    "\n",
    "###Perceptron\n",
    "\n",
    "Perceptron je model a učící algoritmus binárního klasifikátoru, funkce z reálných vstupů $x$ do jedné binární hodnoty $f(x)$:\n",
    "\n",
    "$$f(x) = \\begin{cases}1 & \\text{if }w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases}$$\n",
    "kde $w$ jsou reálné váhy, $w \\cdot x$ je skalární součin $\\sum_{i=0}^m w_i x_i - b$, kde m je počet vstupů, b je práh.\n",
    "\n",
    "\n",
    "Algoritmus učení:\n",
    "\n",
    "1. Inicializuj váhy a prahy malým kladným číslem. \n",
    "\n",
    "2. Pro každý vzor j z tréningové množin D, proveď následující kroky se vstupem  $\\mathbf{x}_j \\,$ a požadovaným výstupem  $d_j \\,$:\n",
    "    - Spočti výstup:\n",
    "$$y_j(t) = f[\\mathbf{w}(t)\\cdot\\mathbf{x}_j] = f[w_0(t) + w_1(t)x_{j,1} + w_2(t)x_{j,2} + \\dotsb + w_n(t)x_{j,n}]$$\n",
    "\n",
    "    - Update vah:\n",
    "$$w_i(t+1) = w_i(t) + \\alpha (d_j - y_j(t)) x_{j,i} \\,, for all feature 0 \\leq i \\leq n.$$\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "###Back Propagation\n",
    "\n",
    "$e_j(n)=d_j(n)-y_j(n)$\n",
    "je chyba j-tého vstupu sítě pro n-tý vzor.\n",
    "Změny vah jsou řízeny snahou minimalizovat chybu:\n",
    "$$\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n).$$\n",
    "\n",
    "$$\\Delta w_{ji} (n) = -\\eta\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$\n",
    "kde $y_i$ je výstup předchozího neuronu a $\\eta$ je krok metody. \n",
    "\n",
    "pro minimalizaci počítáme derivaci\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = e_j(n)\\phi^\\prime (v_j(n))$$\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = \\phi^\\prime (v_j(n))\\sum_k -\\frac{\\partial\\mathcal{E}(n)}{\\partial v_k(n)} w_{kj}(n).$$\n",
    "\n",
    "![svm](mlp.gif)\n",
    "\n",
    "- gradientní descent\n",
    "- stochasticý gradient descent\n",
    "- minibatch gradient descent\n",
    "- weight decay\n",
    "- vanishing gradient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Deep NN\n",
    "\n",
    "Hluboká síť (DNN) je umělá neuronové síť s větším počtem skrytých vrstev. Více vrstev umožňuje agregovat příznaky z předchozích vrstev a tak potenciálně modelovat složité vztahy pomocí menšího celkového množství jednotek v porovnání s mělkými síěmi.\n",
    "\n",
    "DNN jsou nejčastěji dopředné, ale v poslední době se objevují modely rekurentních sítí, jako např. v oblasti zpracování přirozeného jazyka. Konvoluční sítě se používají často pro zpracování obrazu.\n",
    "\n",
    "DNN se nejčastěji učí algoritmem back propagation.\n",
    "\n",
    "$$w_{ij}(t + 1) = w_{ij}(t) + \\eta\\frac{\\partial E}{\\partial w_{ij}}$$\n",
    "\n",
    "kde  $\\eta$ je krok metody a  $E$ je chybová funkce. \n",
    "\n",
    "__Konvolutční sítě:__\n",
    "\n",
    "![](deep.png)\n",
    "\n",
    "- Konvoluční vrstva\n",
    "\n",
    "Na rozdíl od ručně předem zvolených kernelových modelů jsou v konvoluční síti paramtetry jader učeny algoritmem back propagation. Konvolučních jednotek v jedné vrstvě je mnoho a každý kernel je aplikován po celém vzoru (např. obrázku) se stejnými parametry. Konvoluční operátor by měl z dat extrahovat různé příznaky. Typicky se ukazuje, že první vrstvy z obrazových dat získají příznaky jako hrany, rohy, apod, další vrtsvy pak konstruují složitější příznaky.\n",
    "\n",
    "Mějme vrstvu neuronů chápaných dvojrozměrně jako pole $NxN$, nad ní konvolční vrstvu s $m×m$ filtrem $\\omega$. Konvoluční vrstva má výstup velikosti $(N−m+1)×(N−m+1)$. Vstupy do jednotky  $x_{ij}^l$ ve vrstvě jsou tvořeny:\n",
    "$$x_{ij}=\\sum_{a=0}^{m−1}\\sum_{b=0}^{m−1}\\omega_{ab}y^{(l−1)}_{(i+a)(j+b)}.$$\n",
    "\n",
    "\n",
    "- Pooling \n",
    "\n",
    "Pooling jednotky redukují varianci tak, že počítají maximální, průměrnou apod hodnotu nějakého příznaku na lokálním okolí v datech. Tím se dílem zajistí invariance vůči drobným posunutím, dílem se architektura redukuje do pyramidálního tvaru sítě. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Sparsity\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\rho_j = \\frac{1}{m} \\sum_{i=1}^m \\left[ a^{(2)}_j(x^{(i)}) \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\rho_j = \\rho,\n",
    "\\end{align}\n",
    "kde  $\\rho$ je parameter, typicky dost malé kladné číslo (např $\\rho = 0.05$). \n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{j=1}^{s_2} \\rho \\log \\frac{\\rho}{\\hat\\rho_j} + (1-\\rho) \\log \\frac{1-\\rho}{1-\\hat\\rho_j}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "##ReLU\n",
    "\n",
    "\n",
    "Sigmoidální jednotka :\n",
    "$$f(x)=\\frac{1}{1+exp(−x)}$$\n",
    "\n",
    "Tanh jednotka:\n",
    "$$f(x)=tanh(x)$$\n",
    "\n",
    "Rectified linear unit (ReLU):\n",
    "$$f(x)=\\sum_{i=1}^\\infty\n",
    "\\sigma(x−i+0.5) = approx \\log(1+e^x)$$\n",
    "\n",
    "$\\log(1+e^x)$ softplus function $= approx  \\max(0,x+N(0,1))$ (hard max). a to je Rectified Linear Function (ReL).\n",
    " \n",
    " - řeší problém mizení gradientu (gradient je 0 pro $x<0$ a 1 pro $x>1$)\n",
    " \n",
    " - podporuje sparsity ve skrytých vrstvách\n",
    " \n",
    "\n",
    "##Stacked autoencoders\n",
    "\n",
    "\n",
    "autoencoder\n",
    "\n",
    "greedy layer-wise learning\n",
    "\n",
    "po vrstvách, pak doladit - pomáhá řešit problém mizejícího gradientu\n",
    "\n",
    "\n",
    "\n",
    "Yoshua Bengio, Learning Deep Architectures for AI, Foundations and Trends in Machine Learning, 2(1), 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
